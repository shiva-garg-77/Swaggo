# Swaggo Data Science Server\n\nA comprehensive data science and machine learning API server that integrates seamlessly with the main Swaggo website.\n\n## 🚀 Features\n\n- **Data Analysis**: Comprehensive statistical analysis with visualizations\n- **Machine Learning**: Train and deploy ML models with auto-detection\n- **AI Integration**: Connect with existing website AI functionality\n- **File Processing**: Upload and process CSV, JSON, Excel files\n- **Real-time Insights**: AI-powered data insights and recommendations\n- **RESTful API**: Clean API endpoints for all functionality\n- **CORS Enabled**: Seamless integration with frontend applications\n\n## 📋 Requirements\n\n- Python 3.8+\n- Flask and dependencies (see requirements.txt)\n- Node.js (for website integration)\n\n## 🛠️ Installation\n\n### Option 1: Quick Start with Batch File\n\n1. Run the master startup script:\n   ```bash\n   # From the main Swaggo directory\n   run_all_servers.bat\n   ```\n\n### Option 2: Manual Setup\n\n1. Install Python dependencies:\n   ```bash\n   cd ds\n   pip install -r requirements.txt\n   ```\n\n2. Start the data science server:\n   ```bash\n   python app.py\n   ```\n   or\n   ```bash\n   python start_ds_server.py\n   ```\n\n## 🌐 API Endpoints\n\n### Health Check\n```http\nGET /api/health\n```\nReturns server status and available services.\n\n### Data Analysis\n```http\nPOST /api/analyze\nContent-Type: application/json\n\n{\n  \"data\": [\n    {\"name\": \"Alice\", \"age\": 25, \"salary\": 50000},\n    {\"name\": \"Bob\", \"age\": 30, \"salary\": 60000}\n  ]\n}\n```\n\n### Machine Learning Prediction\n```http\nPOST /api/predict\nContent-Type: application/json\n\n{\n  \"data\": {\"age\": 25, \"experience\": 2},\n  \"model_name\": \"salary_predictor\"\n}\n```\n\n### AI Insights\n```http\nPOST /api/ai-insights\nContent-Type: application/json\n\n{\n  \"data\": [...]\n}\n```\n\n### File Upload\n```http\nPOST /api/upload\nContent-Type: multipart/form-data\n\n# Form data with 'file' field containing CSV/JSON/Excel file\n```\n\n### Data Processing\n```http\nPOST /api/process\nContent-Type: application/json\n\n{\n  \"data\": [...]\n}\n```\n\n## 🔧 Configuration\n\nEdit `config/settings.py` to customize:\n\n- Server host and port\n- CORS origins\n- File upload limits\n- ML model parameters\n- Database connections\n\n## 🎯 Usage Examples\n\n### Python Client\n```python\nimport requests\n\n# Analyze data\ndata = [\n    {\"name\": \"Alice\", \"age\": 25, \"salary\": 50000},\n    {\"name\": \"Bob\", \"age\": 30, \"salary\": 60000}\n]\n\nresponse = requests.post(\n    \"http://localhost:5000/api/analyze\",\n    json={\"data\": data}\n)\n\nresult = response.json()\nprint(result[\"analysis\"][\"descriptive_stats\"])\n```\n\n### JavaScript Client\n```javascript\n// Frontend integration\nconst analyzeData = async (data) => {\n  const response = await fetch('http://localhost:5000/api/analyze', {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({ data })\n  });\n  \n  return await response.json();\n};\n```\n\n### cURL Examples\n```bash\n# Health check\ncurl http://localhost:5000/api/health\n\n# Analyze data\ncurl -X POST http://localhost:5000/api/analyze \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"data\": [{\"x\": 1, \"y\": 2}, {\"x\": 3, \"y\": 4}]}'\n\n# Upload file\ncurl -X POST http://localhost:5000/api/upload \\\n  -F \"file=@data.csv\"\n```\n\n## 🔗 Website Integration\n\nThe data science server automatically integrates with your main website:\n\n### Frontend Integration\n- Import the `DataScienceIntegration` component\n- Real-time server status checking\n- Interactive data analysis interface\n\n### Backend Integration\n- GraphQL mutation support\n- User context sharing\n- AI response enhancement\n\n## 📊 Features Detail\n\n### Data Analysis\n- **Descriptive Statistics**: Mean, median, standard deviation, etc.\n- **Missing Value Analysis**: Detection and reporting\n- **Outlier Detection**: IQR and Z-score methods\n- **Correlation Analysis**: Pearson correlation matrices\n- **Data Quality Assessment**: Comprehensive scoring\n- **Visualizations**: Histograms, correlation heatmaps\n\n### Machine Learning\n- **Auto-Detection**: Automatically determines if problem is classification or regression\n- **Multiple Algorithms**: Random Forest, Linear/Logistic Regression, SVM\n- **Model Persistence**: Automatic saving and loading of trained models\n- **Preprocessing**: Automatic feature scaling and encoding\n- **Evaluation Metrics**: Accuracy, R², MSE, classification reports\n\n### AI Integration\n- **Contextual Responses**: Query-aware AI responses\n- **Business Insights**: Revenue, customer, and operational pattern detection\n- **Trend Analysis**: Time-series and correlation insights\n- **Recommendations**: Actionable data-driven suggestions\n\n## 📁 Project Structure\n\n```\nds/\n├── app.py                 # Main Flask application\n├── start_ds_server.py     # Startup script with checks\n├── website_integration.py # Website AI integration\n├── requirements.txt       # Python dependencies\n├── README.md             # This file\n├── services/             # Core services\n│   ├── data_analysis.py  # Data analysis engine\n│   ├── ml_models.py      # Machine learning models\n│   └── ai_integration.py # AI processing service\n├── utils/                # Utilities\n│   └── data_utils.py     # Data processing utilities\n├── config/               # Configuration\n│   └── settings.py       # Server configuration\n├── models/               # Saved ML models (auto-created)\n├── data/                 # Data storage (auto-created)\n└── uploads/              # File uploads (auto-created)\n```\n\n## 🔒 Security Notes\n\n- The server runs with CORS enabled for localhost origins\n- File uploads are limited to specific formats and sizes\n- No authentication is implemented (add as needed for production)\n- Model files are stored locally (consider cloud storage for production)\n\n## 🐛 Troubleshooting\n\n### Common Issues\n\n**Server won't start:**\n- Check if Python 3.8+ is installed\n- Verify all dependencies are installed: `pip install -r requirements.txt`\n- Ensure port 5000 is not in use\n\n**Analysis fails:**\n- Check data format (should be JSON array of objects)\n- Ensure data has at least one numeric column for statistics\n- Verify data is not empty\n\n**Frontend can't connect:**\n- Check if data science server is running on port 5000\n- Verify CORS is properly configured\n- Check browser console for CORS errors\n\n**File upload fails:**\n- Check file size (max 100MB by default)\n- Verify file format (CSV, JSON, Excel supported)\n- Ensure proper form-data encoding\n\n### Debug Mode\n\nStart the server with debug logging:\n```bash\nexport FLASK_ENV=development\npython app.py\n```\n\n## 🚀 Deployment\n\n### Development\n```bash\n# Start all servers together\nrun_all_servers.bat  # Windows\n# or\npython start_all_servers.py  # Cross-platform\n```\n\n### Production\n\n```bash\n# Use Gunicorn for production\ncd ds\npip install gunicorn\ngunicorn -w 4 -b 0.0.0.0:5000 app:app\n```\n\nOr use the production config:\n```python\n# In config/settings.py, set environment to 'production'\napp = create_app('production')\n```\n\n## 🤝 Integration with Main Website\n\nThe data science server is designed to work seamlessly with your existing Swaggo website:\n\n1. **Shared AI Context**: Enhances existing AI responses with data insights\n2. **GraphQL Integration**: Supports mutations for data processing\n3. **Real-time Updates**: Can push insights to frontend components\n4. **User Context**: Accesses user information from main backend\n\n## 📈 Performance\n\n- **Data Processing**: Optimized for datasets up to 100k rows\n- **Model Training**: Supports datasets with up to 50 features\n- **Response Time**: < 2s for typical analysis requests\n- **Memory Usage**: Configurable limits for large datasets\n\n## 🔮 Future Enhancements\n\n- [ ] WebSocket support for real-time streaming\n- [ ] Advanced visualization with Plotly/D3.js\n- [ ] Deep learning model support\n- [ ] Time series analysis\n- [ ] Automated report generation\n- [ ] Database integration for data persistence\n- [ ] Authentication and user management\n- [ ] Distributed computing support\n\n## 📞 Support\n\nFor issues and questions:\n1. Check the troubleshooting section above\n2. Review the API documentation\n3. Check server logs for detailed error messages\n4. Ensure all dependencies are correctly installed\n\n---\n\n**Happy Data Science! 🔬📊🚀**
